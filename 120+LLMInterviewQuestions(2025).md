# 120+ LLM Interview Questions (2025)

## Prompt Engineering & Basics of LLM
- Predictive/Discriminative AI vs Generative AI
- What is LLM, and how are LLMs trained?
- How do LLMs work?
- Tokens in language models
- Key parameters (temperature, top-p, top-k)
- Estimating SaaS vs OSS LLM costs
- Decoding strategies
- Stopping criteria / stop sequences
- In-context learning
- Types of prompt engineering
- Few-shot prompting best practices
- Strategies for good prompts
- Controlling hallucination via prompts
- Improving reasoning with prompt engineering
- If COT fails: alternatives
- Difference from chatbots/statistical models
- Few-shot learning explained

## Retrieval Augmented Generation (RAG)
- How does RAG work?
- RAG pipeline & components
- Benefits of RAG
- Fine-tuning vs RAG
- Architecture patterns for customization
- Evaluating RAG systems
- Customizing LLM with own data

## Document Digitization & Chunking
- What is chunking & why?
- Factors affecting chunk size
- Types of chunking methods
- Finding ideal chunk size
- Handling complex documents
- Handling tables, lists, charts
- Building production-grade pipeline

## Embedding Models
- What are embeddings?
- Short vs long content embeddings
- Benchmarking embedding models
- Improving low-accuracy embeddings
- Improving sentence transformers

## Vector Databases
- Vector DB vs traditional DB
- Vector indexes & search strategies
- Clustering, LSH, random projection, PQ
- Choosing vector indexes & similarity metrics
- Filtering challenges
- Selecting the right DB

## Advanced Search Algorithms
- Information retrieval patterns
- Efficient & accurate search in large datasets
- Fixing inaccurate retrieval in RAG
- Keyword retrieval, re-ranking
- Metrics for IR/recommendation
- Hybrid search & merging results
- Multi-hop queries
- Retrieval improvement techniques

## Language Models: Internal Working
- Self-attention & disadvantages
- Positional encoding
- Transformer architecture
- Seq2Seq models & limits
- Attention functions & multi-head
- Local vs global attention
- Context length extension
- Vocabulary optimization
- Handling long dependencies
- BERT working mechanism
- Different LLM architectures

## Supervised Fine-Tuning
- What is fine-tuning & why?
- Training vs fine-tuning
- When to fine-tune
- Fine-tuning datasets & hyperparameters
- Infra requirements
- Fine-tuning on consumer hardware
- PEFT categories
- Catastrophic forgetting & mitigation
- Overfitting handling

## Preference Alignment (RLHF/DPO)
- RLHF explained
- When to use vs SFT
- Reward hacking
- Other alignment methods

## Evaluation of LLMs
- Evaluating LLM models
- Metrics for LLMs
- Chain of Verification
- Evaluating bias
- Performance evaluation
- LLM limitations

## Hallucination Control
- Types of hallucination
- Control techniques

## Deployment of LLM
- Quantization & accuracy
- Optimizing inference throughput
- Reducing latency
- Large-scale deployment challenges
- Performance vs cost
- Customer service applications

## Agent-Based Systems
- Concepts of agents & strategies
- ReAct prompting
- Plan & Execute strategy
- OpenAI Functions vs LangChain Agents
- Designing agent software
- Role in AGI

## Prompt Hacking
- What is prompt hacking?
- Types
- Defense tactics

## Miscellaneous
- Cost optimization
- Mixture of Experts (MoE)
- Production-grade RAG design
- FP8 variable precision
- Low-precision training
- KV cache size
- Multi-head attention dimensions
- Ensuring attention focus
- Industry applications
- 2025 research trends
- Human-like conversation
- Ethical/responsible AI
- Societal implications
- Explainability improvements
- Handling OOD/nonsensical prompts
- Word2Vec from scratch
- Numeric word representations

## Case Studies
- Chat Assistant with dynamic context
- Prompting Techniques
