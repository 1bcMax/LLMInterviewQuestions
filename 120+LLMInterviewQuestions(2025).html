<!doctype html>
<html>
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta http-equiv="X-UA-Compatible" content="ie=edge" />
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
html {
  font-family: ui-sans-serif, system-ui, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.markmap-dark {
  background: #27272a;
  color: white;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.12/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/index.js"></script><script>((r) => {
          setTimeout(r);
        })(() => {
  const { markmap, mm } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute("style", "position:absolute;bottom:20px;right:20px");
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root2, jsonOptions) => {
              const markmap = getMarkmap();
              window.mm = markmap.Markmap.create(
                "svg#mindmap",
                (getOptions || markmap.deriveOptions)(jsonOptions),
                root2
              );
              if (window.matchMedia("(prefers-color-scheme: dark)").matches) {
                document.documentElement.classList.add("markmap-dark");
              }
            })(() => window.markmap,null,{"content":"120+ LLM Interview Questions (2025)","children":[{"content":"Prompt Engineering &amp; Basics of LLM","children":[{"content":"Predictive/Discriminative AI vs Generative AI","children":[],"payload":{"tag":"li","lines":"3,4"}},{"content":"What is LLM, and how are LLMs trained?","children":[],"payload":{"tag":"li","lines":"4,5"}},{"content":"How do LLMs work?","children":[],"payload":{"tag":"li","lines":"5,6"}},{"content":"Tokens in language models","children":[],"payload":{"tag":"li","lines":"6,7"}},{"content":"Key parameters (temperature, top-p, top-k)","children":[],"payload":{"tag":"li","lines":"7,8"}},{"content":"Estimating SaaS vs OSS LLM costs","children":[],"payload":{"tag":"li","lines":"8,9"}},{"content":"Decoding strategies","children":[],"payload":{"tag":"li","lines":"9,10"}},{"content":"Stopping criteria / stop sequences","children":[],"payload":{"tag":"li","lines":"10,11"}},{"content":"In-context learning","children":[],"payload":{"tag":"li","lines":"11,12"}},{"content":"Types of prompt engineering","children":[],"payload":{"tag":"li","lines":"12,13"}},{"content":"Few-shot prompting best practices","children":[],"payload":{"tag":"li","lines":"13,14"}},{"content":"Strategies for good prompts","children":[],"payload":{"tag":"li","lines":"14,15"}},{"content":"Controlling hallucination via prompts","children":[],"payload":{"tag":"li","lines":"15,16"}},{"content":"Improving reasoning with prompt engineering","children":[],"payload":{"tag":"li","lines":"16,17"}},{"content":"If COT fails: alternatives","children":[],"payload":{"tag":"li","lines":"17,18"}},{"content":"Difference from chatbots/statistical models","children":[],"payload":{"tag":"li","lines":"18,19"}},{"content":"Few-shot learning explained","children":[],"payload":{"tag":"li","lines":"19,21"}}],"payload":{"tag":"h2","lines":"2,3"}},{"content":"Retrieval Augmented Generation (RAG)","children":[{"content":"How does RAG work?","children":[],"payload":{"tag":"li","lines":"22,23"}},{"content":"RAG pipeline &amp; components","children":[],"payload":{"tag":"li","lines":"23,24"}},{"content":"Benefits of RAG","children":[],"payload":{"tag":"li","lines":"24,25"}},{"content":"Fine-tuning vs RAG","children":[],"payload":{"tag":"li","lines":"25,26"}},{"content":"Architecture patterns for customization","children":[],"payload":{"tag":"li","lines":"26,27"}},{"content":"Evaluating RAG systems","children":[],"payload":{"tag":"li","lines":"27,28"}},{"content":"Customizing LLM with own data","children":[],"payload":{"tag":"li","lines":"28,30"}}],"payload":{"tag":"h2","lines":"21,22"}},{"content":"Document Digitization &amp; Chunking","children":[{"content":"What is chunking &amp; why?","children":[],"payload":{"tag":"li","lines":"31,32"}},{"content":"Factors affecting chunk size","children":[],"payload":{"tag":"li","lines":"32,33"}},{"content":"Types of chunking methods","children":[],"payload":{"tag":"li","lines":"33,34"}},{"content":"Finding ideal chunk size","children":[],"payload":{"tag":"li","lines":"34,35"}},{"content":"Handling complex documents","children":[],"payload":{"tag":"li","lines":"35,36"}},{"content":"Handling tables, lists, charts","children":[],"payload":{"tag":"li","lines":"36,37"}},{"content":"Building production-grade pipeline","children":[],"payload":{"tag":"li","lines":"37,39"}}],"payload":{"tag":"h2","lines":"30,31"}},{"content":"Embedding Models","children":[{"content":"What are embeddings?","children":[],"payload":{"tag":"li","lines":"40,41"}},{"content":"Short vs long content embeddings","children":[],"payload":{"tag":"li","lines":"41,42"}},{"content":"Benchmarking embedding models","children":[],"payload":{"tag":"li","lines":"42,43"}},{"content":"Improving low-accuracy embeddings","children":[],"payload":{"tag":"li","lines":"43,44"}},{"content":"Improving sentence transformers","children":[],"payload":{"tag":"li","lines":"44,46"}}],"payload":{"tag":"h2","lines":"39,40"}},{"content":"Vector Databases","children":[{"content":"Vector DB vs traditional DB","children":[],"payload":{"tag":"li","lines":"47,48"}},{"content":"Vector indexes &amp; search strategies","children":[],"payload":{"tag":"li","lines":"48,49"}},{"content":"Clustering, LSH, random projection, PQ","children":[],"payload":{"tag":"li","lines":"49,50"}},{"content":"Choosing vector indexes &amp; similarity metrics","children":[],"payload":{"tag":"li","lines":"50,51"}},{"content":"Filtering challenges","children":[],"payload":{"tag":"li","lines":"51,52"}},{"content":"Selecting the right DB","children":[],"payload":{"tag":"li","lines":"52,54"}}],"payload":{"tag":"h2","lines":"46,47"}},{"content":"Advanced Search Algorithms","children":[{"content":"Information retrieval patterns","children":[],"payload":{"tag":"li","lines":"55,56"}},{"content":"Efficient &amp; accurate search in large datasets","children":[],"payload":{"tag":"li","lines":"56,57"}},{"content":"Fixing inaccurate retrieval in RAG","children":[],"payload":{"tag":"li","lines":"57,58"}},{"content":"Keyword retrieval, re-ranking","children":[],"payload":{"tag":"li","lines":"58,59"}},{"content":"Metrics for IR/recommendation","children":[],"payload":{"tag":"li","lines":"59,60"}},{"content":"Hybrid search &amp; merging results","children":[],"payload":{"tag":"li","lines":"60,61"}},{"content":"Multi-hop queries","children":[],"payload":{"tag":"li","lines":"61,62"}},{"content":"Retrieval improvement techniques","children":[],"payload":{"tag":"li","lines":"62,64"}}],"payload":{"tag":"h2","lines":"54,55"}},{"content":"Language Models: Internal Working","children":[{"content":"Self-attention &amp; disadvantages","children":[],"payload":{"tag":"li","lines":"65,66"}},{"content":"Positional encoding","children":[],"payload":{"tag":"li","lines":"66,67"}},{"content":"Transformer architecture","children":[],"payload":{"tag":"li","lines":"67,68"}},{"content":"Seq2Seq models &amp; limits","children":[],"payload":{"tag":"li","lines":"68,69"}},{"content":"Attention functions &amp; multi-head","children":[],"payload":{"tag":"li","lines":"69,70"}},{"content":"Local vs global attention","children":[],"payload":{"tag":"li","lines":"70,71"}},{"content":"Context length extension","children":[],"payload":{"tag":"li","lines":"71,72"}},{"content":"Vocabulary optimization","children":[],"payload":{"tag":"li","lines":"72,73"}},{"content":"Handling long dependencies","children":[],"payload":{"tag":"li","lines":"73,74"}},{"content":"BERT working mechanism","children":[],"payload":{"tag":"li","lines":"74,75"}},{"content":"Different LLM architectures","children":[],"payload":{"tag":"li","lines":"75,77"}}],"payload":{"tag":"h2","lines":"64,65"}},{"content":"Supervised Fine-Tuning","children":[{"content":"What is fine-tuning &amp; why?","children":[],"payload":{"tag":"li","lines":"78,79"}},{"content":"Training vs fine-tuning","children":[],"payload":{"tag":"li","lines":"79,80"}},{"content":"When to fine-tune","children":[],"payload":{"tag":"li","lines":"80,81"}},{"content":"Fine-tuning datasets &amp; hyperparameters","children":[],"payload":{"tag":"li","lines":"81,82"}},{"content":"Infra requirements","children":[],"payload":{"tag":"li","lines":"82,83"}},{"content":"Fine-tuning on consumer hardware","children":[],"payload":{"tag":"li","lines":"83,84"}},{"content":"PEFT categories","children":[],"payload":{"tag":"li","lines":"84,85"}},{"content":"Catastrophic forgetting &amp; mitigation","children":[],"payload":{"tag":"li","lines":"85,86"}},{"content":"Overfitting handling","children":[],"payload":{"tag":"li","lines":"86,88"}}],"payload":{"tag":"h2","lines":"77,78"}},{"content":"Preference Alignment (RLHF/DPO)","children":[{"content":"RLHF explained","children":[],"payload":{"tag":"li","lines":"89,90"}},{"content":"When to use vs SFT","children":[],"payload":{"tag":"li","lines":"90,91"}},{"content":"Reward hacking","children":[],"payload":{"tag":"li","lines":"91,92"}},{"content":"Other alignment methods","children":[],"payload":{"tag":"li","lines":"92,94"}}],"payload":{"tag":"h2","lines":"88,89"}},{"content":"Evaluation of LLMs","children":[{"content":"Evaluating LLM models","children":[],"payload":{"tag":"li","lines":"95,96"}},{"content":"Metrics for LLMs","children":[],"payload":{"tag":"li","lines":"96,97"}},{"content":"Chain of Verification","children":[],"payload":{"tag":"li","lines":"97,98"}},{"content":"Evaluating bias","children":[],"payload":{"tag":"li","lines":"98,99"}},{"content":"Performance evaluation","children":[],"payload":{"tag":"li","lines":"99,100"}},{"content":"LLM limitations","children":[],"payload":{"tag":"li","lines":"100,102"}}],"payload":{"tag":"h2","lines":"94,95"}},{"content":"Hallucination Control","children":[{"content":"Types of hallucination","children":[],"payload":{"tag":"li","lines":"103,104"}},{"content":"Control techniques","children":[],"payload":{"tag":"li","lines":"104,106"}}],"payload":{"tag":"h2","lines":"102,103"}},{"content":"Deployment of LLM","children":[{"content":"Quantization &amp; accuracy","children":[],"payload":{"tag":"li","lines":"107,108"}},{"content":"Optimizing inference throughput","children":[],"payload":{"tag":"li","lines":"108,109"}},{"content":"Reducing latency","children":[],"payload":{"tag":"li","lines":"109,110"}},{"content":"Large-scale deployment challenges","children":[],"payload":{"tag":"li","lines":"110,111"}},{"content":"Performance vs cost","children":[],"payload":{"tag":"li","lines":"111,112"}},{"content":"Customer service applications","children":[],"payload":{"tag":"li","lines":"112,114"}}],"payload":{"tag":"h2","lines":"106,107"}},{"content":"Agent-Based Systems","children":[{"content":"Concepts of agents &amp; strategies","children":[],"payload":{"tag":"li","lines":"115,116"}},{"content":"ReAct prompting","children":[],"payload":{"tag":"li","lines":"116,117"}},{"content":"Plan &amp; Execute strategy","children":[],"payload":{"tag":"li","lines":"117,118"}},{"content":"OpenAI Functions vs LangChain Agents","children":[],"payload":{"tag":"li","lines":"118,119"}},{"content":"Designing agent software","children":[],"payload":{"tag":"li","lines":"119,120"}},{"content":"Role in AGI","children":[],"payload":{"tag":"li","lines":"120,122"}}],"payload":{"tag":"h2","lines":"114,115"}},{"content":"Prompt Hacking","children":[{"content":"What is prompt hacking?","children":[],"payload":{"tag":"li","lines":"123,124"}},{"content":"Types","children":[],"payload":{"tag":"li","lines":"124,125"}},{"content":"Defense tactics","children":[],"payload":{"tag":"li","lines":"125,127"}}],"payload":{"tag":"h2","lines":"122,123"}},{"content":"Miscellaneous","children":[{"content":"Cost optimization","children":[],"payload":{"tag":"li","lines":"128,129"}},{"content":"Mixture of Experts (MoE)","children":[],"payload":{"tag":"li","lines":"129,130"}},{"content":"Production-grade RAG design","children":[],"payload":{"tag":"li","lines":"130,131"}},{"content":"FP8 variable precision","children":[],"payload":{"tag":"li","lines":"131,132"}},{"content":"Low-precision training","children":[],"payload":{"tag":"li","lines":"132,133"}},{"content":"KV cache size","children":[],"payload":{"tag":"li","lines":"133,134"}},{"content":"Multi-head attention dimensions","children":[],"payload":{"tag":"li","lines":"134,135"}},{"content":"Ensuring attention focus","children":[],"payload":{"tag":"li","lines":"135,136"}},{"content":"Industry applications","children":[],"payload":{"tag":"li","lines":"136,137"}},{"content":"2025 research trends","children":[],"payload":{"tag":"li","lines":"137,138"}},{"content":"Human-like conversation","children":[],"payload":{"tag":"li","lines":"138,139"}},{"content":"Ethical/responsible AI","children":[],"payload":{"tag":"li","lines":"139,140"}},{"content":"Societal implications","children":[],"payload":{"tag":"li","lines":"140,141"}},{"content":"Explainability improvements","children":[],"payload":{"tag":"li","lines":"141,142"}},{"content":"Handling OOD/nonsensical prompts","children":[],"payload":{"tag":"li","lines":"142,143"}},{"content":"Word2Vec from scratch","children":[],"payload":{"tag":"li","lines":"143,144"}},{"content":"Numeric word representations","children":[],"payload":{"tag":"li","lines":"144,146"}}],"payload":{"tag":"h2","lines":"127,128"}},{"content":"Case Studies","children":[{"content":"Chat Assistant with dynamic context","children":[],"payload":{"tag":"li","lines":"147,148"}},{"content":"Prompting Techniques","children":[],"payload":{"tag":"li","lines":"148,149"}}],"payload":{"tag":"h2","lines":"146,147"}}],"payload":{"tag":"h1","lines":"0,1"}},null)</script>
</body>
</html>
